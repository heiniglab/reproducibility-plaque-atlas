{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icb/korbinian.traeuble/miniconda3/envs/jupyter-experimental/lib/python3.10/site-packages/scvi/_settings.py:63: UserWarning: Since v1.0.0, scvi-tools no longer uses a random seed by default. Run `scvi.settings.seed = 0` to reproduce results from previous versions.\n",
      "  self.seed = seed\n",
      "/home/icb/korbinian.traeuble/miniconda3/envs/jupyter-experimental/lib/python3.10/site-packages/scvi/_settings.py:70: UserWarning: Setting `dl_pin_memory_gpu_training` is deprecated in v1.0 and will be removed in v1.1. Please pass in `pin_memory` to the data loaders instead.\n",
      "  self.dl_pin_memory_gpu_training = (\n",
      "/home/icb/korbinian.traeuble/miniconda3/envs/jupyter-experimental/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:In order to use sagenet models, please install pytorch geometric (see https://pytorch-geometric.readthedocs.io) and \n",
      " captum (see https://github.com/pytorch/captum).\n",
      "WARNING:root:mvTCR is not installed. To use mvTCR models, please install it first using \"pip install mvtcr\"\n",
      "WARNING:root:multigrate is not installed. To use multigrate models, please install it first using \"pip install multigrate\".\n",
      "/home/icb/korbinian.traeuble/miniconda3/envs/jupyter-experimental/lib/python3.10/site-packages/scanpy/_settings.py:450: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  IPython.display.set_matplotlib_formats(*ipython_format)\n",
      "/home/icb/korbinian.traeuble/miniconda3/envs/jupyter-experimental/lib/python3.10/site-packages/umap/__init__.py:9: ImportWarning: Tensorflow not installed; ParametricUMAP will be unavailable\n",
      "/home/icb/korbinian.traeuble/miniconda3/envs/jupyter-experimental/lib/python3.10/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import anndata\n",
    "import os\n",
    "from scipy.sparse import issparse\n",
    "import scvi\n",
    "import scib\n",
    "from scarches.models.scpoli import scPoli\n",
    "import scarches as sca\n",
    "from harmony import harmonize\n",
    "import pyliger\n",
    "from scarches.dataset.trvae.data_handling import remove_sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook the annotated datasets are concatinated, a ensembl <-> gene name mapping dictionary is created and different integration methods are implemented/executed on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping file for ensembl genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final = sc.read_h5ad(\"../data/Plaque-datasets/small-datasets-concat.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids_fernandez = pd.read_csv(\"/Users/korbinian.traeuble/PhD-local/projects/main_Roche/data/Plaque-datasets/Fernandez/GSE224273/GSE224273_RAW/Sample1/genes.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use column \"1\" as index and rename column \"0\" to \"ensembl_id\"\n",
    "gene_ids_fernandez = gene_ids_fernandez.set_index(1).rename(columns={0: \"ensembl_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the index (gene name) and count the number of occurrences\n",
    "grouped = gene_ids_fernandez.groupby(gene_ids_fernandez.index).size()\n",
    "\n",
    "# Filter to get only those groups that have more than one occurrence\n",
    "duplicates = grouped[grouped > 1]\n",
    "\n",
    "# Get the actual rows from gene_ids_fernandez for these duplicates\n",
    "duplicate_rows = gene_ids_fernandez.loc[duplicates.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names_adata = adata_final.var.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gene_ids = []\n",
    "for gene_name in gene_names_adata:\n",
    "    try:\n",
    "        ensembl_ids = gene_ids_fernandez.loc[gene_name]['ensembl_id']\n",
    "        # If ensembl_ids is a Series (multiple values), you can choose the first one\n",
    "        ensembl_id = ensembl_ids.iloc[0] if isinstance(ensembl_ids, pd.Series) else ensembl_ids\n",
    "    except KeyError:\n",
    "        ensembl_id = 'not found'\n",
    "    fixed_gene_ids.append(ensembl_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gene_ids.count(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of gene names that are not found\n",
    "gene_names_not_found_fernandez = [gene_name for gene_name, ensembl_id in zip(gene_names_adata, fixed_gene_ids) if ensembl_id == \"not found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gene_names_not_found_fernandez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe out of gene_names_adata and fixed_gene_ids\n",
    "df_fernandez = pd.DataFrame({\"gene_name\": gene_names_adata, \"ensembl_id\": fixed_gene_ids})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids_slysz = pd.read_csv(\"/Users/korbinian.traeuble/PhD-local/projects/main_Roche/tables/slysz_features.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use column \"1\" as index and rename column \"0\" to \"ensembl_id\"\n",
    "gene_ids_slysz = gene_ids_slysz.set_index(1).rename(columns={0: \"ensembl_id\"}).drop(columns=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gene_ids_slysz = []\n",
    "for gene_name in gene_names_not_found_fernandez:\n",
    "    try:\n",
    "        ensembl_ids = gene_ids_slysz.loc[gene_name]['ensembl_id']\n",
    "        # If ensembl_ids is a Series (multiple values), you can choose the first one\n",
    "        ensembl_id = ensembl_ids.iloc[0] if isinstance(ensembl_ids, pd.Series) else ensembl_ids\n",
    "    except KeyError:\n",
    "        ensembl_id = 'not found'\n",
    "    fixed_gene_ids_slysz.append(ensembl_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gene_ids_slysz.count(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of gene names that are not found\n",
    "gene_names_not_found_slysz = [gene_name for gene_name, ensembl_id in zip(gene_names_not_found_fernandez, fixed_gene_ids_slysz) if ensembl_id == \"not found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gene_names_not_found_slysz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe out of gene_names_not_found_fernandez and fixed_gene_ids_slysz\n",
    "df_slysz = pd.DataFrame({\"gene_name\": gene_names_not_found_fernandez, \"ensembl_id\": fixed_gene_ids_slysz})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids_alsaigh = pd.read_csv(\"/Users/korbinian.traeuble/PhD-local/projects/main_Roche/tables/alsaigh_features.tsv\", sep=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_ids_alsaigh= gene_ids_alsaigh.set_index(1).rename(columns={0: \"ensembl_id\"}).drop(columns=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gene_ids_alsaigh = []\n",
    "for gene_name in gene_names_not_found_slysz:\n",
    "    try:\n",
    "        ensembl_ids = gene_ids_alsaigh.loc[gene_name]['ensembl_id']\n",
    "        # If ensembl_ids is a Series (multiple values), you can choose the first one\n",
    "        ensembl_id = ensembl_ids.iloc[0] if isinstance(ensembl_ids, pd.Series) else ensembl_ids\n",
    "    except KeyError:\n",
    "        ensembl_id = 'not found'\n",
    "    fixed_gene_ids_alsaigh.append(ensembl_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_gene_ids_alsaigh.count(\"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names_not_found_alsaigh = [gene_name for gene_name, ensembl_id in zip(gene_names_not_found_slysz, fixed_gene_ids_alsaigh) if ensembl_id == \"not found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe out of gene_names_not_found_slysz and fixed_gene_ids_alsaigh\n",
    "df_alsaigh = pd.DataFrame({\"gene_name\": gene_names_not_found_slysz, \"ensembl_id\": fixed_gene_ids_alsaigh})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bring df together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the \"not founds\" from df_fernandez, df_slysz, but not df_alsaigh and merge all three\n",
    "df_fernandez = df_fernandez[df_fernandez[\"ensembl_id\"] != \"not found\"]\n",
    "df_slysz = df_slysz[df_slysz[\"ensembl_id\"] != \"not found\"]\n",
    "\n",
    "\n",
    "df_merged = pd.concat([df_fernandez, df_slysz, df_alsaigh])\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the 'not founds' in ensemble_id of df_merged\n",
    "df_merged[\"ensembl_id\"].value_counts()[\"not found\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_merged to csv\n",
    "df_merged.to_csv(\"../tables/gene_names_to_ensembl_ids.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check leftover genes. found out its due to seurat and scanpy make unique gene names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene in gene_names_not_found_alsaigh:\n",
    "    for dataset in adata_list:\n",
    "        if gene in dataset.var_names:\n",
    "            print(f\"{gene} found in {dataset.obs['dataset'][0]}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get \"not found\" ensemble ids\n",
    "not_found_ensembl_ids = df_merged[df_merged[\"ensembl_id\"] == \"not found\"][\"gene_name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene_name in not_found_ensembl_ids:\n",
    "    gene_name_shortened = gene_name[:-2]\n",
    "    if gene_name_shortened in gene_ids_alsaigh.index:\n",
    "        ensembl_ids = gene_ids_alsaigh.loc[gene_name_shortened]['ensembl_id']\n",
    "        ensembl_id = ensembl_ids.iloc[1] if isinstance(ensembl_ids, pd.Series) else ensembl_ids\n",
    "        df_merged.loc[df_merged[\"gene_name\"] == gene_name, \"ensembl_id\"] = ensembl_id\n",
    "        print(f\"Replaced {gene_name} with {ensembl_id}\")\n",
    "    else:\n",
    "        print(f\"{gene_name} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get \"not found\" ensemble ids\n",
    "not_found_ensembl_ids_post = df_merged[df_merged[\"ensembl_id\"] == \"not found\"][\"gene_name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_ensembl_ids_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 5 left not found gene ids are in fact not .1 suffixed by scanpy/seurat. Looked up the gene ids manually and replaced them in the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_lookup_dict = {\n",
    "    \"RP11-442N24--B.1\": \"ENSG00000229388\",\n",
    "    \"RP11-524D16--A.3\": \"ENSG00000261295\",\n",
    "    \"RP11-59D5--B.2\": \"ENSG00000236345\",\n",
    "    \"RP11-99J16--A.2\": \"ENSG00000244137\",\n",
    "    \"XXyac-YX65C7-A.2\": \"ENSG00000226445\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the not founds in df_merged with the manual lookup dict\n",
    "for gene_name, ensembl_id in manual_lookup_dict.items():\n",
    "    df_merged.loc[df_merged[\"gene_name\"] == gene_name, \"ensembl_id\"] = ensembl_id\n",
    "    print(f\"Replaced {gene_name} with {ensembl_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get \"not found\" ensemble ids\n",
    "not_found_ensembl_ids_post_post = df_merged[df_merged[\"ensembl_id\"] == \"not found\"][\"gene_name\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove Unnamed: 0 in df_merged\n",
    "df_merged = df_merged.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df_merged to csv\n",
    "df_merged.to_csv(\"/Users/korbinian.traeuble/PhD-local/projects/main_Roche/tables/gene_names_to_ensembl_ids_ALLFOUND.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaque_datasets_path = \"/lustre/groups/epigenereg01/workspace/projects/Plaque-atlas/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_alsaigh = os.path.join(plaque_datasets_path, \"Alsaigh/Alsaigh_annot_all_withsubclusters_withdoublets.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsaigh = sc.read_h5ad(path_alsaigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T cell          13069\n",
       "Macrophage       7553\n",
       "EC               2918\n",
       "SMC              2665\n",
       "unknown          1812\n",
       "Fibroblast       1561\n",
       "B cell           1558\n",
       "NK               1239\n",
       "Monocyte          921\n",
       "Fibromyocyte      821\n",
       "Mast cell         487\n",
       "Plasma cell       281\n",
       "doublets          100\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alsaigh.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_wirka = os.path.join(plaque_datasets_path, \"Wirka/Wirka_annot_5678.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wirka = sc.read_h5ad(path_wirka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EC              1494\n",
       "Macrophage      1335\n",
       "Fibroblast      1137\n",
       "SMC             1134\n",
       "unknown         1093\n",
       "T cell           475\n",
       "B cell           473\n",
       "Fibromyocyte     453\n",
       "Plasma cell      205\n",
       "NK               101\n",
       "Monocyte          78\n",
       "DC                64\n",
       "Mast              31\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wirka.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_slysz6 = os.path.join(plaque_datasets_path, \"Slysz/Slysz_fem_sample6_annot.h5ad\")\n",
    "path_slysz8 = os.path.join(plaque_datasets_path, \"Slysz/Slysz_fem_sample8_annot.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "slysz6 = sc.read_h5ad(path_slysz6)\n",
    "slysz8 = sc.read_h5ad(path_slysz8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T cell         6672\n",
       "NK              892\n",
       "B cell          435\n",
       "Macrophage      286\n",
       "Monocytes       278\n",
       "Mast cell       183\n",
       "unknown          64\n",
       "DC               34\n",
       "Plasma cell      17\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slysz6.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T cell         4148\n",
       "B cell         3785\n",
       "Monocyte       1138\n",
       "NK              307\n",
       "doublets        210\n",
       "Macrophage      169\n",
       "Plasma cell      94\n",
       "Mast             49\n",
       "unknown          21\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slysz8.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_pan2 = os.path.join(plaque_datasets_path, \"Pan/Pan_sample2_annot.h5ad\")\n",
    "path_pan3 = os.path.join(plaque_datasets_path, \"Pan/Pan_sample3_annot.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pan2 = sc.read_h5ad(path_pan2)\n",
    "pan3 = sc.read_h5ad(path_pan3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Macrophage     714\n",
       "EC             701\n",
       "SMC            593\n",
       "unknown        315\n",
       "DC             311\n",
       "T cell         299\n",
       "Fibroblast     272\n",
       "Mast cell      132\n",
       "Monocyte       105\n",
       "Plasma cell     25\n",
       "B cell          19\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pan2.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SMC             708\n",
       "Macrophage      513\n",
       "EC              445\n",
       "T cell          333\n",
       "Fibromyocyte    289\n",
       "DC              217\n",
       "unknown         116\n",
       "NK               54\n",
       "Mast cell        46\n",
       "Monocyte         34\n",
       "B cell           12\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pan3.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path_roche = os.path.join(plaque_datasets_path, \"Lars_Roche2/Roche_annot.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roche = sc.read_h5ad(path_roche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cell_type_level1\n",
       "T cell          292\n",
       "EC              207\n",
       "Macrophage      173\n",
       "Fibromyocyte    165\n",
       "SMC             144\n",
       "unknown         136\n",
       "NK               77\n",
       "Fibroblast       30\n",
       "Plasma cell      24\n",
       "Mast cell        18\n",
       "B cell           12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#roche.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_list = [alsaigh, wirka, slysz6, slysz8, pan2, pan3]# , roche]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AnnData object with n_obs × n_vars = 34985 × 22742\n",
       "     obs: 'sample', 'dataset', 'symptoms', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'size_factors', 'leiden', 'cell_type_level1', 'leiden_subclustering'\n",
       "     obsm: 'X_pca', 'X_umap', 'decontX_1_UMAP', 'decontX_2_UMAP', 'decontX_3_UMAP'\n",
       "     layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts',\n",
       " AnnData object with n_obs × n_vars = 8073 × 20431\n",
       "     obs: 'cell_type', 'sample', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'dataset', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'symptoms', 'size_factors', 'n_genes', 'leiden', 'cell_type_level1'\n",
       "     obsm: 'X_pca', 'X_umap', 'decontX_1_UMAP', 'decontX_2_UMAP', 'decontX_3_UMAP', 'decontX_4_UMAP', 'decontX_5_UMAP', 'decontX_6_UMAP', 'decontX_7_UMAP', 'decontX_8_UMAP'\n",
       "     layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts',\n",
       " AnnData object with n_obs × n_vars = 8861 × 26071\n",
       "     obs: 'sample', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'n_counts', 'dataset', 'symptoms', 'size_factors', 'leiden', 'cell_type_level1'\n",
       "     var: 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "     uns: 'X_name', 'cell_type_level1_colors', 'decontX', 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'scDblFinder.class_colors', 'scDblFinder.threshold', 'umap'\n",
       "     obsm: 'X_pca', 'X_umap', 'decontX_1_2_3_UMAP', 'decontX_4_UMAP', 'decontX_5_UMAP', 'decontX_6_UMAP', 'decontX_7_UMAP', 'decontX_8_UMAP', 'decontX_9_UMAP'\n",
       "     varm: 'PCs'\n",
       "     layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts'\n",
       "     obsp: 'connectivities', 'distances',\n",
       " AnnData object with n_obs × n_vars = 9921 × 26071\n",
       "     obs: 'sample', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'n_counts', 'dataset', 'symptoms', 'size_factors', 'leiden', 'cell_type_level1'\n",
       "     var: 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts', 'n_cells', 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "     uns: 'X_name', 'cell_type_level1_colors', 'decontX', 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'scDblFinder.class_colors', 'scDblFinder.threshold', 'umap'\n",
       "     obsm: 'X_pca', 'X_umap', 'decontX_1_2_3_UMAP', 'decontX_4_UMAP', 'decontX_5_UMAP', 'decontX_6_UMAP', 'decontX_7_UMAP', 'decontX_8_UMAP', 'decontX_9_UMAP'\n",
       "     varm: 'PCs'\n",
       "     layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts'\n",
       "     obsp: 'connectivities', 'distances',\n",
       " AnnData object with n_obs × n_vars = 3486 × 17818\n",
       "     obs: 'sample', 'symptoms', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'dataset', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'size_factors', 'leiden', 'cell_type_level1'\n",
       "     var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "     uns: 'X_name', 'cell_type_level1_colors', 'decontX', 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'scDblFinder.class_colors', 'scDblFinder.threshold', 'umap'\n",
       "     obsm: 'X_pca', 'X_umap', 'decontX_1_UMAP', 'decontX_2_UMAP', 'decontX_3_UMAP'\n",
       "     varm: 'PCs'\n",
       "     layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts'\n",
       "     obsp: 'connectivities', 'distances',\n",
       " AnnData object with n_obs × n_vars = 2767 × 17818\n",
       "     obs: 'sample', 'symptoms', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'dataset', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'size_factors', 'leiden', 'cell_type_level1'\n",
       "     var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n",
       "     uns: 'X_name', 'cell_type_level1_colors', 'decontX', 'hvg', 'leiden', 'leiden_colors', 'log1p', 'neighbors', 'pca', 'scDblFinder.class_colors', 'scDblFinder.threshold', 'umap'\n",
       "     obsm: 'X_pca', 'X_umap', 'decontX_1_UMAP', 'decontX_2_UMAP', 'decontX_3_UMAP'\n",
       "     varm: 'PCs'\n",
       "     layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts'\n",
       "     obsp: 'connectivities', 'distances',\n",
       " AnnData object with n_obs × n_vars = 1278 × 22310\n",
       "     obs: 'dataset', 'cell_type', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'sample', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'cell_type_level1'\n",
       "     var: 'mt', 'n_cells_by_counts', 'mean_counts', 'pct_dropout_by_counts', 'total_counts'\n",
       "     uns: 'X_name', 'decontX', 'scDblFinder.threshold'\n",
       "     obsm: 'decontX_2_UMAP', 'decontX_3_UMAP', 'decontX_4_UMAP', 'decontX_5_UMAP', 'decontX_6_UMAP', 'decontX_7_UMAP', 'decontX_8_UMAP', 'decontX_9_UMAP'\n",
       "     layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final = anndata.concat(adata_list, join='outer', fill_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T cell          24996\n",
       "Macrophage      10570\n",
       "B cell           6282\n",
       "EC               5558\n",
       "SMC              5100\n",
       "unknown          3421\n",
       "Fibroblast       2970\n",
       "NK               2593\n",
       "Monocyte         2276\n",
       "Fibromyocyte     1563\n",
       "Mast cell         848\n",
       "DC                626\n",
       "Plasma cell       622\n",
       "doublets          310\n",
       "Monocytes         278\n",
       "Mast               80\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_final.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Monocytes to Monocyte and Mast to Mast cell\n",
    "adata_final.obs[\"cell_type_level1\"] = adata_final.obs[\"cell_type_level1\"].replace(\"Monocytes\", \"Monocyte\")\n",
    "adata_final.obs[\"cell_type_level1\"] = adata_final.obs[\"cell_type_level1\"].replace(\"Mast\", \"Mast cell\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T cell          24996\n",
       "Macrophage      10570\n",
       "B cell           6282\n",
       "EC               5558\n",
       "SMC              5100\n",
       "unknown          3421\n",
       "Fibroblast       2970\n",
       "NK               2593\n",
       "Monocyte         2554\n",
       "Fibromyocyte     1563\n",
       "Mast cell         928\n",
       "DC                626\n",
       "Plasma cell       622\n",
       "doublets          310\n",
       "Name: cell_type_level1, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#no neurons\n",
    "adata_final.obs[\"cell_type_level1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 68093 × 31347\n",
       "    obs: 'sample', 'dataset', 'symptoms', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'size_factors', 'leiden', 'cell_type_level1', 'leiden_subclustering', 'cell_type', 'n_counts'\n",
       "    obsm: 'X_pca', 'X_umap', 'decontX_1_UMAP', 'decontX_2_UMAP', 'decontX_3_UMAP', 'decontX_4_UMAP', 'decontX_5_UMAP', 'decontX_6_UMAP', 'decontX_7_UMAP', 'decontX_8_UMAP', 'decontX_1_2_3_UMAP', 'decontX_9_UMAP'\n",
       "    layers: 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final.write_h5ad(\"small-datasets-concat-noRoche.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.1 Gene name mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembl_id_df = pd.read_csv(\"gene_names_to_ensembl_ALLFOUND_allfernandez_no6_withallslysz.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from gene names to Ensembl IDs\n",
    "gene_to_ensembl = dict(zip(ensembl_id_df['gene_name'], ensembl_id_df['ensembl_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the variable names in AnnData\n",
    "adata_final.var['original_gene_names'] = adata_final.var_names\n",
    "adata_final.var_names = [gene_to_ensembl[gene] if gene in gene_to_ensembl else gene for gene in adata_final.var_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_ENSG_vars = adata_final.var_names[~adata_final.var_names.str.startswith('ENSG')]\n",
    "len(non_ENSG_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save .X in a new layer\n",
    "adata_final.layers[\"log1p_scran_samplewise\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the rounded_corrected_counts layer as .X\n",
    "adata.X = adata.layers[\"rounded_corrected_counts\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final.write_h5ad(\"small-datasets-concat-ensembl-noRoche.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(68093, 31347)\n",
      "(68093, 31347)\n"
     ]
    }
   ],
   "source": [
    "# Duplicate cells (before hvg)\n",
    "print(adata_final.layers[\"uncorrected_counts\"].toarray().shape)\n",
    "print(np.unique(adata_final.layers[\"uncorrected_counts\"].toarray(), axis=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.2 Gene names aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"small-datasets-concat-ensembl-noRoche.h5ad\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggregation\n",
    "# Convert the sparse matrix (if it is sparse) to a dense DataFrame\n",
    "adata_df = pd.DataFrame(adata.X.toarray() if issparse(adata.X) else adata.X, \n",
    "                        index=adata.obs_names, \n",
    "                        columns=adata.var_names)\n",
    "\n",
    "# Group by gene names and sum the counts\n",
    "aggregated_data = adata_df.groupby(adata_df.columns, axis=1).sum()\n",
    "\n",
    "# Prepare the new 'var' DataFrame, keeping the first occurrence of each gene\n",
    "unique_var = adata.var.loc[~adata.var.index.duplicated(keep='first')]\n",
    "\n",
    "# Create a new AnnData object with aggregated data\n",
    "adata_agg = anndata.AnnData(X=aggregated_data, obs=adata.obs, var=unique_var.loc[aggregated_data.columns])\n",
    "\n",
    "# 'adata_agg' now has unique gene names and aggregated counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 68093 × 31347\n",
       "    obs: 'sample', 'dataset', 'symptoms', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'size_factors', 'leiden', 'cell_type_level1', 'leiden_subclustering', 'cell_type', 'n_counts'\n",
       "    var: 'original_gene_names'\n",
       "    obsm: 'X_pca', 'X_umap', 'decontX_1_2_3_UMAP', 'decontX_1_UMAP', 'decontX_2_UMAP', 'decontX_3_UMAP', 'decontX_4_UMAP', 'decontX_5_UMAP', 'decontX_6_UMAP', 'decontX_7_UMAP', 'decontX_8_UMAP', 'decontX_9_UMAP'\n",
       "    layers: 'log1p_scran_samplewise', 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 68093 × 27441\n",
       "    obs: 'sample', 'dataset', 'symptoms', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'size_factors', 'leiden', 'cell_type_level1', 'leiden_subclustering', 'cell_type', 'n_counts'\n",
       "    var: 'original_gene_names'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The counts are correctly summed: True\n"
     ]
    }
   ],
   "source": [
    "# MANUALLY CHECK IF SUMMATION WORKED AS INTENDED\n",
    "# Extract counts for \"ENSG00000033050\" from the original adata\n",
    "original_counts = adata[:, \"ENSG00000033050\"].X\n",
    "if issparse(original_counts):\n",
    "    original_counts = original_counts.toarray()  # Convert to dense array if sparse\n",
    "\n",
    "# Sum these counts cellwise\n",
    "summed_counts = np.sum(original_counts, axis=1)\n",
    "\n",
    "# Extract counts for \"ENSG00000033050\" from adata_agg\n",
    "agg_counts = adata_agg[:, \"ENSG00000033050\"].X\n",
    "if issparse(agg_counts):\n",
    "    agg_counts = agg_counts.toarray()  # Convert to dense array if sparse\n",
    "\n",
    "# Compare the two sets of counts\n",
    "comparison = np.allclose(summed_counts.flatten(), agg_counts.flatten())\n",
    "\n",
    "# Print the result of the comparison\n",
    "print(f\"The counts are correctly summed: {comparison}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adata_agg.write_h5ad(\"small-datasets-concat-aggr-X.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating layer: log1p_scran_samplewise\n",
      "Aggregating layer: raw_decontXcounts\n",
      "Aggregating layer: rounded_corrected_counts\n",
      "Aggregating layer: uncorrected_counts\n"
     ]
    }
   ],
   "source": [
    "# Function to aggregate a layer\n",
    "def aggregate_layer(layer):\n",
    "    layer_df = pd.DataFrame(layer.toarray() if issparse(layer) else layer, \n",
    "                            index=adata.obs_names, \n",
    "                            columns=adata.var_names)\n",
    "    return layer_df.groupby(layer_df.columns, axis=1).sum()\n",
    "\n",
    "# Aggregate each layer and add to adata_agg\n",
    "for layer_name in adata.layers.keys():\n",
    "    print(f\"Aggregating layer: {layer_name}\")\n",
    "    aggregated_layer = aggregate_layer(adata.layers[layer_name])\n",
    "    \n",
    "    # The first time, we need to initialize layers in adata_agg\n",
    "    if not hasattr(adata_agg, 'layers'):\n",
    "        adata_agg.layers = {}\n",
    "\n",
    "    # Add the aggregated layer to adata_agg\n",
    "    adata_agg.layers[layer_name] = aggregated_layer\n",
    "\n",
    "# Now 'adata_agg' contains all the aggregated layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking layer: log1p_scran_samplewise\n",
      "The counts are correctly summed in layer log1p_scran_samplewise: True\n",
      "Checking layer: raw_decontXcounts\n",
      "The counts are correctly summed in layer raw_decontXcounts: True\n",
      "Checking layer: rounded_corrected_counts\n",
      "The counts are correctly summed in layer rounded_corrected_counts: True\n",
      "Checking layer: uncorrected_counts\n",
      "The counts are correctly summed in layer uncorrected_counts: True\n"
     ]
    }
   ],
   "source": [
    "def sum_gene_counts(layer, gene_name, var_names):\n",
    "    # Find the index(es) of the gene\n",
    "    gene_indices = np.where(var_names == gene_name)[0]\n",
    "\n",
    "    # Sum counts across all occurrences of the gene\n",
    "    gene_counts = np.sum(layer[:, gene_indices].toarray(), axis=1) if issparse(layer) else np.sum(layer[:, gene_indices], axis=1)\n",
    "    return gene_counts\n",
    "\n",
    "# Check for each layer\n",
    "for layer_name in adata.layers.keys():\n",
    "    print(f\"Checking layer: {layer_name}\")\n",
    "\n",
    "    # Get var names for the current layer\n",
    "    var_names = adata.var_names\n",
    "\n",
    "    # Sum counts for \"ENSG00000033050\" in the original data\n",
    "    original_counts = sum_gene_counts(adata.layers[layer_name], \"ENSG00000033050\", var_names)\n",
    "\n",
    "    # Extract counts for \"ENSG00000033050\" from adata_agg layer\n",
    "    gene_index_agg = np.where(adata_agg.var_names == \"ENSG00000033050\")[0]\n",
    "    agg_counts = adata_agg.layers[layer_name][:, gene_index_agg]\n",
    "    if issparse(agg_counts):\n",
    "        agg_counts = agg_counts.toarray()\n",
    "\n",
    "    # Compare the two sets of counts\n",
    "    comparison = np.allclose(original_counts.flatten(), agg_counts.flatten())\n",
    "\n",
    "    # Print the result of the comparison\n",
    "    print(f\"The counts are correctly summed in layer {layer_name}: {comparison}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 68093 × 27441\n",
       "    obs: 'sample', 'dataset', 'symptoms', 'scDblFinder.sample', 'scDblFinder.class', 'scDblFinder.score', 'scDblFinder.weighted', 'scDblFinder.cxds_score', 'decontX_contamination', 'decontX_clusters', 'n_genes_by_counts', 'total_counts', 'total_counts_mt', 'pct_counts_mt', 'n_genes', 'size_factors', 'leiden', 'cell_type_level1', 'leiden_subclustering', 'cell_type', 'n_counts'\n",
       "    var: 'original_gene_names'\n",
       "    layers: 'log1p_scran_samplewise', 'raw_decontXcounts', 'rounded_corrected_counts', 'uncorrected_counts'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting to sparse matrix in log1p_scran_samplewise\n",
      "converting to sparse matrix in raw_decontXcounts\n",
      "converting to sparse matrix in rounded_corrected_counts\n",
      "converting to sparse matrix in uncorrected_counts\n"
     ]
    }
   ],
   "source": [
    "adata_agg.X = sparse.csr_matrix(adata_agg.X)\n",
    "for layer_name in adata_agg.layers.keys():\n",
    "    print(\"converting to sparse matrix in\", layer_name)\n",
    "    adata_agg.layers[layer_name] = sparse.csr_matrix(adata_agg.layers[layer_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_agg.write_h5ad(\"small-datasets-concat-aggr-all-sparse-noRoche.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final = sc.read_h5ad(\"small-datasets-concat-aggr-all-sparse-noRoche.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform a clustering for scran normalization in clusters\n",
    "adata_pp = adata_final.copy()\n",
    "sc.pp.normalize_total(adata_pp, target_sum=1e6)\n",
    "sc.pp.log1p(adata_pp)\n",
    "sc.pp.pca(adata_pp, svd_solver=\"arpack\")\n",
    "sc.pp.neighbors(adata_pp, n_pcs=30)\n",
    "sc.tl.leiden(adata_pp, key_added='groups', resolution=0.22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.rinterface_lib.callbacks\n",
    "import logging\n",
    "\n",
    "from rpy2.robjects import pandas2ri\n",
    "import anndata2ri\n",
    "\n",
    "# Ignore R warning messages\n",
    "#Note: this can be commented out to get more verbose R output\n",
    "rpy2.rinterface_lib.callbacks.logger.setLevel(logging.ERROR)\n",
    "\n",
    "# Automatically convert rpy2 outputs to pandas dataframes\n",
    "pandas2ri.activate()\n",
    "anndata2ri.activate()\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess variables for scran normalization\n",
    "input_groups = adata_pp.obs['groups']\n",
    "data_mat = adata_final.X.T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i data_mat -i input_groups -o size_factors\n",
    "library(scran)\n",
    "size_factors = calculateSumFactors(data_mat, clusters=input_groups, min.mean=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del adata_pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final.obs['size_factors'] = size_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize adata \n",
    "adata_final.X /= adata_final.obs['size_factors'].values[:,None]\n",
    "sc.pp.log1p(adata_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final.write_h5ad(\"small-datasets-concat-aggr-all-sparse-scranlog1p-noRoche.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final = sc.read_h5ad(\"small-datasets-concat-aggr-all-sparse-scranlog1p-noRoche.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_adata(adata_final):\n",
    "    \n",
    "    #delete all layers that are not needed to reduce memory\n",
    "    del adata_final.layers['log1p_scran_samplewise'] #only for small integrations\n",
    "    del adata_final.layers['raw_decontXcounts']\n",
    "    del adata_final.layers['uncorrected_counts']\n",
    "\n",
    "    #rename layer to counts\n",
    "    adata_final.layers['counts'] = adata_final.layers['rounded_corrected_counts']\n",
    "    del adata_final.layers['rounded_corrected_counts']\n",
    "\n",
    "    # add \"_{dataset}\" to the sample name to make them unique\n",
    "    adata_final.obs[\"sample\"] = adata_final.obs[\"sample\"].astype(str) + \"_\" + adata_final.obs[\"dataset\"].astype(str)\n",
    "    adata_final.obs['sample'] = adata_final.obs['sample'].astype('category')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_adata(adata_final, mode, batch_key):\n",
    "\n",
    "    # remove unknown and doublets\n",
    "    print(\"Removing unknowns and doublets...\")\n",
    "    adata_final = adata_final[~adata_final.obs['cell_type_level1'].isin(['unknown', 'doublets'])].copy()\n",
    "\n",
    "    \n",
    "    if mode==\"auto\":\n",
    "        print(\"Using auto mode to do hvg and pca...\")\n",
    "        scib.preprocessing.reduce_data(adata_final, batch_key=batch_key)\n",
    "        \n",
    "    elif mode==\"manual\":\n",
    "        print(\"Using manual mode to do hvg and pca...\")\n",
    "        sc.pp.highly_variable_genes(adata_final,  n_top_genes = 2000, batch_key = batch_key)\n",
    "        sc.pp.pca(adata_final, use_highly_variable=True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be 'auto' or 'manual'\")\n",
    "\n",
    "    \n",
    "    # subset only hvgs\n",
    "    print(\"Subset for HVGs...\")\n",
    "    hvg = adata_final.var[adata_final.var['highly_variable']].index.tolist()\n",
    "    adata_final = adata_final[:, hvg].copy()\n",
    "\n",
    "    \n",
    "    #check how many cells have zero counts for all genes\n",
    "    print(\"Check how many cells have zero counts for all genes...\")\n",
    "    cellwise_sum = adata_final.X.sum(axis=1)\n",
    "    num_cells_zero_counts = (cellwise_sum == 0).sum()\n",
    "    \n",
    "    if num_cells_zero_counts>0:\n",
    "        print(num_cells_zero_counts, \" cells were found with 0 counts across all genes! Removing these cells now...\")\n",
    "        adata_final = adata_final[cellwise_sum > 0, :]\n",
    "\n",
    "    \n",
    "    # check for dublicate gene expressions across cells after HVG selection\n",
    "    print(\"Check for dublicate gene expressions\")\n",
    "    before = adata_final.layers[\"counts\"].toarray().shape[0]\n",
    "    after = np.unique(adata_final.layers[\"counts\"].toarray(), axis=0).shape[0]\n",
    "    diff = before - after\n",
    "\n",
    "    if diff > 0:\n",
    "        print(diff, \" Non unique cell expression profiles found! Removing them...\")\n",
    "        \n",
    "        counts_array = adata_final.layers[\"counts\"].toarray()\n",
    "        # Find the indices of unique rows\n",
    "        _, unique_indices = np.unique(counts_array, axis=0, return_index=True)\n",
    "        # Sort the indices to maintain the original order\n",
    "        unique_indices_sorted = np.sort(unique_indices)\n",
    "        # Filter the AnnData object to keep only unique rows\n",
    "        adata_final = adata_final[unique_indices_sorted]\n",
    "    else:\n",
    "        \"No non unique cells found.\"\n",
    "\n",
    "    print(\"Preprocessing finished!\")  \n",
    "\n",
    "    return adata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_methods(adata, batch_key, cell_type, scgen_impl):\n",
    "    \n",
    "    # adata has to be filtered, hvg filtered and pca applied in obsm[X_pca]\n",
    "    # log normalized data in .X and counts in .layers[\"counts\"] \n",
    "\n",
    "    \n",
    "    adata_final = adata\n",
    "\n",
    "    # scGen\n",
    "    print(\"Integrating with scGen...\")\n",
    "\n",
    "    if scgen_impl == \"scib\":\n",
    "        print(\"scGen scib implementation\")\n",
    "        adata_after = scib.ig.scgen(adata_final, batch=batch_key, cell_type=cell_type)\n",
    "        adata_final.obsm[\"scGen\"] = adata_after.obsm[\"corrected_latent\"].copy()\n",
    "        \n",
    "    elif scgen_impl == \"scarches\":\n",
    "        print(\"scGen scarches implementation\")\n",
    "\n",
    "        adata_final = remove_sparsity(adata_final) # remove sparsity\n",
    "        adata_final.X = adata_final.X.astype(\"float32\")\n",
    "        \n",
    "        epoch = 100\n",
    "        early_stopping_kwargs = {\n",
    "            \"early_stopping_metric\": \"val_loss\",\n",
    "            \"patience\": 25,\n",
    "            \"threshold\": 0,\n",
    "            \"reduce_lr\": True,\n",
    "            \"lr_patience\": 13,\n",
    "            \"lr_factor\": 0.1,\n",
    "        }\n",
    "\n",
    "        network = sca.models.scgen(adata = adata_final,\n",
    "                           hidden_layer_sizes=[256,128])\n",
    "\n",
    "        network.train(n_epochs=epoch, early_stopping_kwargs = early_stopping_kwargs, batch_size = 32)\n",
    "\n",
    "        adata_after = network.batch_removal(adata_final, batch_key=batch_key, cell_label_key=cell_type,return_latent=True)\n",
    "        adata_final.obsm[\"scGen\"] = adata_after.obsm[\"corrected_latent\"].copy()\n",
    "    \n",
    "    \n",
    "    # scVI\n",
    "    print(\"Integrating with scVI...\")\n",
    "    \n",
    "    scvi.model.SCVI.setup_anndata(adata_final, layer=\"counts\", batch_key=batch_key)\n",
    "    vae = scvi.model.SCVI(adata_final, gene_likelihood=\"nb\", n_layers=2, n_latent=30)\n",
    "    vae.train()\n",
    "    adata_final.obsm[\"scVI\"] = vae.get_latent_representation()\n",
    "\n",
    "    \n",
    "    # scANVI\n",
    "    print(\"Integrating with scANVI...\")\n",
    "    \n",
    "    lvae = scvi.model.SCANVI.from_scvi_model(\n",
    "        vae,\n",
    "        #adata=adata_final,\n",
    "        labels_key=cell_type,\n",
    "        unlabeled_category=\"none\"\n",
    "    )\n",
    "    lvae.train(max_epochs=40)\n",
    "    adata_final.obsm[\"scANVI\"] = lvae.get_latent_representation()\n",
    "\n",
    "    \n",
    "    # scPoli\n",
    "    print(\"Integrating with scPoli...\")\n",
    "    early_stopping_kwargs = {\n",
    "    \"early_stopping_metric\": \"val_prototype_loss\",\n",
    "    \"mode\": \"min\",\n",
    "    \"threshold\": 0,\n",
    "    \"patience\": 20,\n",
    "    \"reduce_lr\": True,\n",
    "    \"lr_patience\": 13,\n",
    "    \"lr_factor\": 0.1,\n",
    "    }\n",
    "    \n",
    "    scpoli_model = scPoli(\n",
    "    adata=adata_final,\n",
    "    condition_keys=batch_key,\n",
    "    cell_type_keys=cell_type,\n",
    "    embedding_dims=5,\n",
    "    recon_loss='nb',\n",
    "    )\n",
    "    \n",
    "    scpoli_model.train(\n",
    "        n_epochs=50,\n",
    "        pretraining_epochs=40,\n",
    "        early_stopping_kwargs=early_stopping_kwargs,\n",
    "        eta=5,\n",
    "    )\n",
    "\n",
    "    scpoli_latent = scpoli_model.get_latent(adata_final)\n",
    "    adata_final.obsm[\"scPoli\"] = scpoli_latent\n",
    "\n",
    "    \n",
    "    # Harmony\n",
    "    print(\"Integrating with Harmony...\")\n",
    "    adata_final.obsm[\"Harmony\"] = harmonize(adata_final.obsm[\"X_pca\"], adata_final.obs, batch_key=batch_key)\n",
    "\n",
    "    \n",
    "    # LIGER\n",
    "    print(\"Integrating with LIGER...\")\n",
    "\n",
    "    batch_cats = adata_final.obs[\"sample\"].cat.categories\n",
    "    bdata = adata_final.copy()\n",
    "    # Pyliger normalizes by library size with a size factor of 1\n",
    "    # So here we give it the count data\n",
    "    bdata.X = bdata.layers[\"counts\"]\n",
    "    # List of adata per dataset\n",
    "    adata_list = [bdata[bdata.obs[batch_key] == b].copy() for b in batch_cats]\n",
    "    for i, ad in enumerate(adata_list):\n",
    "        ad.uns[\"sample_name\"] = batch_cats[i]\n",
    "        # Hack to make sure each method uses the same genes\n",
    "        ad.uns[\"var_gene_idx\"] = np.arange(bdata.n_vars)\n",
    "    \n",
    "    \n",
    "    liger_data = pyliger.create_liger(adata_list, remove_missing=False, make_sparse=False)\n",
    "    # Hack to make sure each method uses the same genes\n",
    "    liger_data.var_genes = bdata.var_names\n",
    "    pyliger.normalize(liger_data)\n",
    "    pyliger.scale_not_center(liger_data)\n",
    "    pyliger.optimize_ALS(liger_data, k=30)\n",
    "    pyliger.quantile_norm(liger_data)\n",
    "    \n",
    "    \n",
    "    adata_final.obsm[\"LIGER\"] = np.zeros((adata_final.shape[0], liger_data.adata_list[0].obsm[\"H_norm\"].shape[1]))\n",
    "    for i, b in enumerate(batch_cats):\n",
    "        adata_final.obsm[\"LIGER\"][adata_final.obs[batch_key] == b] = liger_data.adata_list[i].obsm[\"H_norm\"]\n",
    "\n",
    "    print(\"Finished integrating the data\")\n",
    "    \n",
    "    return adata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_sca(adata, batch_key, cell_type,):\n",
    "    \n",
    "    # adata has to be filtered, hvg filtered and pca applied in obsm[X_pca]\n",
    "    # log normalized data in .X and counts in .layers[\"counts\"] \n",
    "\n",
    "    \n",
    "    adata_final = adata\n",
    "\n",
    "    adata_counts = adata_final.copy()\n",
    "    adata_counts.X = adata_counts.layers[\"counts\"].copy()\n",
    "\n",
    "    print(\"scGen scib implementation\")\n",
    "    adata_after = scib.ig.scgen(adata_final, batch=batch_key, cell_type=cell_type)\n",
    "    adata_final.obsm[\"scGen\"] = adata_after.obsm[\"corrected_latent\"].copy()\n",
    "    \n",
    "    '''\n",
    "    # scGen too slow\n",
    "    print(\"Integrating with scGen...\")\n",
    "\n",
    "    adata_final = remove_sparsity(adata_final) # remove sparsity\n",
    "    adata_final.X = adata_final.X.astype(\"float32\")\n",
    "    \n",
    "    epoch = 100\n",
    "    early_stopping_kwargs = {\n",
    "        \"early_stopping_metric\": \"val_loss\",\n",
    "        \"patience\": 25,\n",
    "        \"threshold\": 0,\n",
    "        \"reduce_lr\": True,\n",
    "        \"lr_patience\": 13,\n",
    "        \"lr_factor\": 0.1,\n",
    "    }\n",
    "\n",
    "    network = sca.models.scgen(adata = adata_final,\n",
    "                       hidden_layer_sizes=[256,128])\n",
    "\n",
    "    network.train(n_epochs=epoch, early_stopping_kwargs = early_stopping_kwargs, batch_size = 32)\n",
    "\n",
    "    adata_after = network.batch_removal(adata_final, batch_key=batch_key, cell_label_key=cell_type,return_latent=True)\n",
    "    adata_final.obsm[\"scGen\"] = adata_after.obsm[\"corrected_latent\"].copy()\n",
    "    '''\n",
    "\n",
    "    \n",
    "    # scVI\n",
    "    print(\"Integrating with scVI...\")\n",
    "\n",
    "    sca.models.SCVI.setup_anndata(adata_counts, batch_key=batch_key)\n",
    "\n",
    "    vae = sca.models.SCVI(\n",
    "        adata_counts,\n",
    "        n_layers=2,\n",
    "        encode_covariates=True,\n",
    "        deeply_inject_covariates=False,\n",
    "        use_layer_norm=\"both\",\n",
    "        use_batch_norm=\"none\",\n",
    "    )\n",
    "\n",
    "    vae.train()\n",
    "    adata_final.obsm[\"scVI\"] = vae.get_latent_representation()\n",
    "\n",
    "    \n",
    "    # scANVI\n",
    "    print(\"Integrating with scANVI...\")\n",
    "\n",
    "    scanvae = sca.models.SCANVI.from_scvi_model(vae, unlabeled_category = \"none\", labels_key=cell_type)\n",
    "    scanvae.train(max_epochs=40)\n",
    "    adata_final.obsm[\"scANVI\"] = scanvae.get_latent_representation()\n",
    "\n",
    "    \n",
    "    \n",
    "    # scPoli\n",
    "    print(\"Integrating with scPoli...\")\n",
    "    early_stopping_kwargs = {\n",
    "    \"early_stopping_metric\": \"val_prototype_loss\",\n",
    "    \"mode\": \"min\",\n",
    "    \"threshold\": 0,\n",
    "    \"patience\": 20,\n",
    "    \"reduce_lr\": True,\n",
    "    \"lr_patience\": 13,\n",
    "    \"lr_factor\": 0.1,\n",
    "    }\n",
    "    \n",
    "    scpoli_model = scPoli(\n",
    "    adata=adata_final,\n",
    "    condition_keys=batch_key,\n",
    "    cell_type_keys=cell_type,\n",
    "    embedding_dims=5,\n",
    "    recon_loss='mse',\n",
    "    )\n",
    "    \n",
    "    scpoli_model.train(\n",
    "        n_epochs=50,\n",
    "        pretraining_epochs=40,\n",
    "        early_stopping_kwargs=early_stopping_kwargs,\n",
    "        eta=5,\n",
    "    )\n",
    "\n",
    "    scpoli_latent = scpoli_model.get_latent(adata_final)\n",
    "    adata_final.obsm[\"scPoli\"] = scpoli_latent\n",
    "\n",
    "\n",
    "    '''\n",
    "    ####################################################################################\n",
    "    # scPoli counts\n",
    "    print(\"Integrating with scPoli using raw counts...\")\n",
    "    early_stopping_kwargs = {\n",
    "    \"early_stopping_metric\": \"val_prototype_loss\",\n",
    "    \"mode\": \"min\",\n",
    "    \"threshold\": 0,\n",
    "    \"patience\": 20,\n",
    "    \"reduce_lr\": True,\n",
    "    \"lr_patience\": 13,\n",
    "    \"lr_factor\": 0.1,\n",
    "    }\n",
    "    \n",
    "    scpoli_model2 = scPoli(\n",
    "    adata=adata_counts,\n",
    "    condition_keys=batch_key,\n",
    "    cell_type_keys=cell_type,\n",
    "    embedding_dims=5,\n",
    "    recon_loss='nb',\n",
    "    )\n",
    "    \n",
    "    scpoli_model2.train(\n",
    "        n_epochs=50,\n",
    "        pretraining_epochs=40,\n",
    "        early_stopping_kwargs=early_stopping_kwargs,\n",
    "        eta=5,\n",
    "    )\n",
    "\n",
    "    scpoli_latent2 = scpoli_model2.get_latent(adata_counts)\n",
    "    adata_final.obsm[\"scPoli_counts\"] = scpoli_latent2\n",
    "    ####################################################################################\n",
    "    '''\n",
    "    \n",
    "    # Harmony\n",
    "    print(\"Integrating with Harmony...\")\n",
    "    adata_final.obsm[\"Harmony\"] = harmonize(adata_final.obsm[\"X_pca\"], adata_final.obs, batch_key=batch_key)\n",
    "\n",
    "    \n",
    "    # LIGER\n",
    "    print(\"Integrating with LIGER...\")\n",
    "\n",
    "    batch_cats = adata_final.obs[\"sample\"].cat.categories\n",
    "    bdata = adata_final.copy()\n",
    "    # Pyliger normalizes by library size with a size factor of 1\n",
    "    # So here we give it the count data\n",
    "    bdata.X = bdata.layers[\"counts\"]\n",
    "    # List of adata per dataset\n",
    "    adata_list = [bdata[bdata.obs[batch_key] == b].copy() for b in batch_cats]\n",
    "    for i, ad in enumerate(adata_list):\n",
    "        ad.uns[\"sample_name\"] = batch_cats[i]\n",
    "        # Hack to make sure each method uses the same genes\n",
    "        ad.uns[\"var_gene_idx\"] = np.arange(bdata.n_vars)\n",
    "    \n",
    "    \n",
    "    liger_data = pyliger.create_liger(adata_list, remove_missing=False, make_sparse=False)\n",
    "    # Hack to make sure each method uses the same genes\n",
    "    liger_data.var_genes = bdata.var_names\n",
    "    pyliger.normalize(liger_data)\n",
    "    pyliger.scale_not_center(liger_data)\n",
    "    pyliger.optimize_ALS(liger_data, k=30)\n",
    "    pyliger.quantile_norm(liger_data)\n",
    "    \n",
    "    \n",
    "    adata_final.obsm[\"LIGER\"] = np.zeros((adata_final.shape[0], liger_data.adata_list[0].obsm[\"H_norm\"].shape[1]))\n",
    "    for i, b in enumerate(batch_cats):\n",
    "        adata_final.obsm[\"LIGER\"][adata_final.obs[batch_key] == b] = liger_data.adata_list[i].obsm[\"H_norm\"]\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Finished integrating the data\")\n",
    "    \n",
    "    return adata_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_adata(adata_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing unknowns and doublets...\n",
      "Using auto mode to do hvg and pca...\n",
      "HVG\n",
      "Using 55 HVGs from full intersect set\n",
      "Using 91 HVGs from n_batch-1 set\n",
      "Using 166 HVGs from n_batch-2 set\n",
      "Using 249 HVGs from n_batch-3 set\n",
      "Using 208 HVGs from n_batch-4 set\n",
      "Using 254 HVGs from n_batch-5 set\n",
      "Using 315 HVGs from n_batch-6 set\n",
      "Using 426 HVGs from n_batch-7 set\n",
      "Using 236 HVGs from n_batch-8 set\n",
      "Using 2000 HVGs\n",
      "Computed 2000 highly variable genes\n",
      "PCA\n",
      "Nearest Neigbours\n",
      "Subset for HVGs...\n",
      "Check how many cells have zero counts for all genes...\n",
      "17  cells were found with 0 counts across all genes! Removing these cells now...\n",
      "Check for dublicate gene expressions\n",
      "4  Non unique cell expression profiles found! Removing them...\n",
      "Preprocessing finished!\n"
     ]
    }
   ],
   "source": [
    "adata_final = preprocess_adata(adata_final, mode = \"auto\", batch_key=\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final = integrate_sca(adata_final, batch_key = \"sample\", cell_type = \"cell_type_level1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 2707.6715998649597 seconds\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()\n",
    "print(f\"Elapsed time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for dublicates in embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  for  Harmony\n",
      "0  for  LIGER\n",
      "0  for  X_pca\n",
      "0  for  scGen\n",
      "0  for  scANVI\n",
      "0  for  scPoli\n",
      "0  for  scVI\n"
     ]
    }
   ],
   "source": [
    "for embed in ['Harmony', 'LIGER', 'X_pca', \"scGen\", 'scANVI', 'scPoli', 'scVI']:\n",
    "    diff = adata_final.obsm[embed].shape[0] - np.unique(adata_final.obsm[embed], axis=0).shape[0]\n",
    "    print(diff,\" for \", embed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename obsm X_pca to PCA in adata_final\n",
    "adata_final.obsm[\"PCA\"] = adata_final.obsm[\"X_pca\"]\n",
    "del adata_final.obsm[\"X_pca\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_final.write_h5ad(\"scarches-noroche-mse.h5ad\") # used in the 3-1_evaluation-HPC.py script"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
